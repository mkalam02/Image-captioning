{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a05fd1-817e-4e33-afb0-b411f9eb1d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\mkalam\\ML_projs\\image_captioning\n"
     ]
    }
   ],
   "source": [
    "# ==== CONFIG ====\n",
    "from pathlib import Path\n",
    "import random, os, numpy as np, tensorflow as tf\n",
    "\n",
    "class C:\n",
    "    # --- paths\n",
    "    ROOT   = Path.home() / \"ML_projs\" / \"image_captioning\"\n",
    "    DATA   = ROOT / \"data\"\n",
    "    RAW    = DATA / \"raw\"\n",
    "    IMAGES = DATA / \"images\" / \"flickr8k\"\n",
    "    ANN    = DATA / \"annotations\"\n",
    "    PROC   = DATA / \"processed\"\n",
    "    MODELS = ROOT / \"models\"\n",
    "    APP    = ROOT / \"app\"\n",
    "\n",
    "    # --- encoder/decoder\n",
    "    IMG_SIZE   = 224\n",
    "    FEAT_DIM   = 1280        # 1280 for EfficientNetB0, 2048 if you used ResNet50\n",
    "    EMB_DIM    = 256\n",
    "    LSTM_UNITS = 512\n",
    "\n",
    "    # --- tokenizer\n",
    "    MIN_FREQ = 4             # you can tune this\n",
    "    MAX_LEN  = 20            # from step 2.3 (override if recomputed)\n",
    "\n",
    "    # --- training\n",
    "    EPOCHS         = 20\n",
    "    BATCH_SIZE     = 64\n",
    "    DROPOUT        = 0.4\n",
    "    LR             = 1e-3\n",
    "    SEED           = 42\n",
    "\n",
    "# make sure folders exist\n",
    "for p in [C.DATA, C.RAW, C.IMAGES, C.ANN, C.PROC, C.MODELS, C.APP]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(C.SEED)\n",
    "np.random.seed(C.SEED)\n",
    "tf.random.set_seed(C.SEED)\n",
    "\n",
    "print(\"Project root:\", C.ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccd3146f-4994-4883-861b-0c6725c0c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle CLI installed. Config dir: C:\\Users\\mkalam\\.kaggle\n"
     ]
    }
   ],
   "source": [
    "# 1) Install kaggle in THIS environment\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaggle\"])\n",
    "\n",
    "# 2) Ensure Kaggle config dir is your home .kaggle\n",
    "from pathlib import Path\n",
    "import os\n",
    "kaggle_dir = Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = str(kaggle_dir)\n",
    "\n",
    "print(\"Kaggle CLI installed. Config dir:\", kaggle_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ec9865-d266-44aa-a31f-18f92ecfa3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle package version: unknown\n",
      "Python exe: C:\\Users\\mkalam\\anaconda3\\envs\\cap_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import kaggle, sys\n",
    "print(\"kaggle package version:\", getattr(kaggle, \"__version__\", \"unknown\"))\n",
    "print(\"Python exe:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90255365-d2ae-44fd-9c48-6970cab09a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
      "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/mkalam/ML_projs/image_captioning/data/raw/flickr8k.zip'),\n",
       " WindowsPath('C:/Users/mkalam/ML_projs/image_captioning/data/raw/Images')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "RAW  = C.RAW\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # reads C:\\Users\\mkalam\\.kaggle\\kaggle.json\n",
    "\n",
    "# Download the dataset zip to RAW (don't unzip yet)\n",
    "api.dataset_download_files(\n",
    "    dataset='adityajn105/flickr8k',\n",
    "    path=str(RAW),\n",
    "    unzip=False,\n",
    "    quiet=False\n",
    ")\n",
    "\n",
    "list(RAW.iterdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2893efeb-7439-4d62-b4b7-1bc1500cf5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Images in: C:\\Users\\mkalam\\ML_projs\\image_captioning\\data\\images\\flickr8k\n",
      "Captions in: C:\\Users\\mkalam\\ML_projs\\image_captioning\\data\\annotations\n"
     ]
    }
   ],
   "source": [
    "import zipfile, shutil\n",
    "\n",
    "IMAGES = C.IMAGES\n",
    "ANN    = C.ANN\n",
    "IMAGES.mkdir(parents=True, exist_ok=True)\n",
    "ANN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Unzip every zip we just downloaded to RAW\n",
    "for z in RAW.glob(\"*.zip\"):\n",
    "    with zipfile.ZipFile(z, \"r\") as f:\n",
    "        f.extractall(RAW)\n",
    "\n",
    "# Move images (usually under a folder named 'Images')\n",
    "src_images_dir = RAW / \"Images\"\n",
    "if src_images_dir.exists():\n",
    "    for p in src_images_dir.iterdir():\n",
    "        if p.is_file():\n",
    "            p.replace(IMAGES / p.name)\n",
    "\n",
    "# Move captions file (dataset provides one of these)\n",
    "for name in (\"captions.txt\", \"Flickr8k.token.txt\"):\n",
    "    f = RAW / name\n",
    "    if f.exists():\n",
    "        f.replace(ANN / f.name)\n",
    "\n",
    "print(\"Done. Images in:\", IMAGES)\n",
    "print(\"Captions in:\", ANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32e475c8-d782-4983-89ab-615f5e618a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 8091\n",
      "captions.txt: True\n",
      "Flickr8k.token.txt: False\n"
     ]
    }
   ],
   "source": [
    "n_imgs = len(list(C.IMAGES.glob(\"*.jpg\"))) + len(list(C.IMAGES.glob(\"*.jpeg\"))) + len(list(C.IMAGES.glob(\"*.png\")))\n",
    "print(\"Image count:\", n_imgs)\n",
    "print(\"captions.txt:\", (C.ANN/\"captions.txt\").exists())\n",
    "print(\"Flickr8k.token.txt:\", (C.ANN/\"Flickr8k.token.txt\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83981418-49cf-4c19-ab5e-2a8d2304db1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8091,\n",
       " ['1000268201_693b08cb0e.jpg',\n",
       "  '1001773457_577c3a7d70.jpg',\n",
       "  '1002674143_1b742ab4b8.jpg'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, json\n",
    "from collections import defaultdict\n",
    "\n",
    "IMAGES = C.IMAGES\n",
    "ANN = C.ANN\n",
    "PROC = C.PROC\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cap_file = ANN / \"captions.txt\"   # you have this one\n",
    "assert cap_file.exists(), \"captions.txt not found\"\n",
    "\n",
    "def basic_clean(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9' ]+\", \" \", s)   # keep letters, digits, apostrophes, spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Build {image_name: [captions...]} with start/end tokens\n",
    "captions = defaultdict(list)\n",
    "with open(cap_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line: \n",
    "            continue\n",
    "        # Format is \"image_name,caption\"\n",
    "        if \",\" not in line:\n",
    "            continue\n",
    "        img, cap = line.split(\",\", 1)\n",
    "        if not (IMAGES / img).exists():\n",
    "            continue\n",
    "        clean = basic_clean(cap)\n",
    "        if clean:\n",
    "            captions[img].append(f\"startseq {clean} endseq\")\n",
    "\n",
    "# drop any images without valid captions\n",
    "captions = {k: v for k, v in captions.items() if v}\n",
    "\n",
    "with open(PROC / \"captions_clean.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(captions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "len(captions), list(captions.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cce2b66-91bd-4cc0-ad82-2c6f00dde152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image: 1000268201_693b08cb0e.jpg\n",
      "Number of captions: 5\n",
      "Example caption: startseq a child in a pink dress is climbing up a set of stairs in an entry way endseq\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(PROC / \"captions_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "sample_img = list(captions.keys())[0]\n",
    "print(\"Sample image:\", sample_img)\n",
    "print(\"Number of captions:\", len(captions[sample_img]))\n",
    "print(\"Example caption:\", captions[sample_img][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01dd7cce-093d-4e57-9153-44f04234bc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 8091\n",
      "Train: 6472  Val: 809  Test: 810\n",
      "Overlap train/val: 0\n",
      "Overlap train/test: 0\n",
      "Overlap val/test: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "PROC = C.PROC\n",
    "\n",
    "# Load the cleaned captions dictionary we saved in 2.1\n",
    "with open(PROC / \"captions_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "# Get the list of image filenames that have captions\n",
    "image_ids = sorted(captions.keys())\n",
    "\n",
    "# Split: 80% train, 20% temp; then split temp into 50/50 = 10% val, 10% test\n",
    "train_ids, temp_ids = train_test_split(\n",
    "    image_ids, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "val_ids, test_ids = train_test_split(\n",
    "    temp_ids, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Total images: {len(image_ids)}\")\n",
    "print(f\"Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}\")\n",
    "\n",
    "# Save the splits for future steps\n",
    "for name, ids in [(\"train\", train_ids), (\"val\", val_ids), (\"test\", test_ids)]:\n",
    "    with open(PROC / f\"{name}_images.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines([img + \"\\n\" for img in ids])\n",
    "\n",
    "# Quick sanity check: make sure all ids are unique and disjoint\n",
    "overlap_train_val = set(train_ids) & set(val_ids)\n",
    "overlap_train_test = set(train_ids) & set(test_ids)\n",
    "overlap_val_test   = set(val_ids) & set(test_ids)\n",
    "print(\"Overlap train/val:\", len(overlap_train_val))\n",
    "print(\"Overlap train/test:\", len(overlap_train_test))\n",
    "print(\"Overlap val/test:\", len(overlap_val_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a00bb144-2592-48d8-a537-78d552a3979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocabulary size: 3063\n",
      "Max caption length (95th percentile): 20\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json, pickle\n",
    "\n",
    "PROC = C.PROC\n",
    "\n",
    "#  Load cleaned captions + training image IDs\n",
    "with open(PROC / \"captions_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    captions = json.load(f)\n",
    "with open(PROC / \"train_images.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_ids = [x.strip() for x in f]\n",
    "\n",
    "#  Count word frequencies (from TRAIN captions only)\n",
    "freq = Counter()\n",
    "for img in train_ids:\n",
    "    for cap in captions[img]:\n",
    "        freq.update(cap.split())\n",
    "\n",
    "#  Keep words that appear at least a few times (to remove noise)\n",
    "min_freq = C.MIN_FREQ      # tweakable; lower = larger vocab\n",
    "vocab = [w for w, c in freq.items() if c >= min_freq]\n",
    "\n",
    "# Add special tokens\n",
    "specials = [\"<pad>\", \"<unk>\"]  # pad for short captions, unk for unknown words\n",
    "itos = specials + sorted(vocab)   # index→string\n",
    "stoi = {w: i for i, w in enumerate(itos)}  # string→index\n",
    "\n",
    "# Find a practical maximum caption length\n",
    "lengths = []\n",
    "for img in train_ids:\n",
    "    for cap in captions[img]:\n",
    "        lengths.append(len(cap.split()))\n",
    "lengths.sort()\n",
    "max_len = lengths[int(0.95 * len(lengths))]  # 95th percentile cutoff\n",
    "\n",
    "# Save tokenizer info\n",
    "with open(PROC / \"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"itos\": itos, \"stoi\": stoi, \"max_len\": int(max_len)}, f)\n",
    "\n",
    "print(\"✅ Vocabulary size:\", len(itos))\n",
    "print(\"Max caption length (95th percentile):\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a6870a9-ad2d-492a-b4f7-ae1485c52a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6472 809 810\n",
      "Encoder: efficientnetb0 | feature dim: 1280\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "IMAGES = C.IMAGES\n",
    "PROC   = C.PROC\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Load split lists ----\n",
    "def read_list(name):\n",
    "    with open(PROC / f\"{name}_images.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "train_ids = read_list(\"train\")\n",
    "val_ids   = read_list(\"val\")\n",
    "test_ids  = read_list(\"test\")\n",
    "\n",
    "print(len(train_ids), len(val_ids), len(test_ids))\n",
    "\n",
    "# ---- Choose encoder + preprocessing ----\n",
    "enc_name = \"efficientnetb0\"\n",
    "IMG_SIZE = C.IMG_SIZE\n",
    "\n",
    "try:\n",
    "    # EfficientNetB0 (preferred)\n",
    "    from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "    base = EfficientNetB0(include_top=False, weights=\"imagenet\", pooling=\"avg\",\n",
    "                          input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocess_fn = preprocess_input\n",
    "    feat_dim = base.output_shape[-1]  # 1280\n",
    "except Exception as e:\n",
    "    print(\"EfficientNetB0 not available, falling back to ResNet50:\", e)\n",
    "    from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "    base = ResNet50(include_top=False, weights=\"imagenet\", pooling=\"avg\",\n",
    "                    input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocess_fn = preprocess_input\n",
    "    feat_dim = base.output_shape[-1]  # 2048\n",
    "\n",
    "base.trainable = False\n",
    "print(\"Encoder:\", enc_name, \"| feature dim:\", feat_dim)\n",
    "\n",
    "# ---- tf.data loader ----\n",
    "def build_ds(id_list, batch=64, shuffle=False):\n",
    "    paths = [str(IMAGES / img) for img in id_list]\n",
    "\n",
    "    def _load(path):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)        # force RGB\n",
    "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = preprocess_fn(img)                           # model-specific normalization\n",
    "        return img, path\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(paths), 2000), reshuffle_each_iteration=False)\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = build_ds(train_ids, batch=64, shuffle=False)\n",
    "val_ds   = build_ds(val_ids,   batch=64, shuffle=False)\n",
    "test_ds  = build_ds(test_ids,  batch=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52283322-1934-4c2d-9e7e-2c88425295db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "  train: (6472, 1280) -> C:\\Users\\mkalam\\ML_projs\\image_captioning\\data\\processed\\features_b0_train.npz\n",
      "  val  : (809, 1280) -> C:\\Users\\mkalam\\ML_projs\\image_captioning\\data\\processed\\features_b0_val.npz\n",
      "  test : (810, 1280) -> C:\\Users\\mkalam\\ML_projs\\image_captioning\\data\\processed\\features_b0_test.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_and_save(ds, id_list, out_path):\n",
    "    feats = []\n",
    "    names = []\n",
    "    for batch_imgs, batch_paths in ds:\n",
    "        batch_feats = base(batch_imgs, training=False).numpy()   # [B, feat_dim]\n",
    "        feats.append(batch_feats)\n",
    "        names.extend([p.numpy().decode(\"utf-8\") for p in batch_paths])\n",
    "\n",
    "    feats = np.vstack(feats)                                     # [N, feat_dim]\n",
    "    # Convert absolute paths back to just the filenames to keep files portable\n",
    "    img_names = [Path(p).name for p in names]\n",
    "    np.savez_compressed(out_path, features=feats, filenames=np.array(img_names))\n",
    "    return feats.shape\n",
    "\n",
    "train_shape = extract_and_save(train_ds, train_ids, C.PROC / \"features_b0_train.npz\")\n",
    "val_shape   = extract_and_save(val_ds,   val_ids,   C.PROC / \"features_b0_val.npz\")\n",
    "test_shape  = extract_and_save(test_ds,  test_ids,  C.PROC / \"features_b0_test.npz\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  train:\", train_shape, \"->\", C.PROC / \"features_b0_train.npz\")\n",
    "print(\"  val  :\", val_shape,   \"->\", C.PROC / \"features_b0_val.npz\")\n",
    "print(\"  test :\", test_shape,  \"->\", C.PROC / \"features_b0_test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4041cc7-fc36-4faf-8e12-42b55af7d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6472, 1280) (6472,)\n",
      "Example: 3393152604_27bd1037f2.jpg → [0.2557082  0.3907099  0.08294499 0.4675278  0.44833222]\n"
     ]
    }
   ],
   "source": [
    "chk = np.load(C.PROC / \"features_b0_train.npz\")\n",
    "print(chk[\"features\"].shape, chk[\"filenames\"].shape)\n",
    "print(\"Example:\", chk[\"filenames\"][0], \"→\", chk[\"features\"][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b1a1319-3532-4a93-9274-29c7a3fd485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3063 | Max length: 20\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "PROC = C.PROC\n",
    "\n",
    "# Load tokenizer info (from Step 2.3)\n",
    "with open(PROC / \"tokenizer.pkl\", \"rb\") as f:\n",
    "    tok_data = pickle.load(f)\n",
    "itos, stoi, max_len = tok_data[\"itos\"], tok_data[\"stoi\"], tok_data[\"max_len\"]\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size, \"| Max length:\", max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ddf1fb2-aad5-4ab4-9244-38f920854dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_features (InputLayer  [(None, 1280)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 256)                  327936    ['image_features[0][0]']      \n",
      "                                                                                                  \n",
      " text_seq (InputLayer)       [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVec  (None, 20, 256)              0         ['dense_2[0][0]']             \n",
      " tor)                                                                                             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 20, 256)              784128    ['text_seq[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 20, 512)              0         ['repeat_vector_1[0][0]',     \n",
      " )                                                                   'embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 20, 512)              2099200   ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 20, 512)              0         ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 20, 3063)             1571319   ['dropout_1[0][0]']           \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4782583 (18.24 MB)\n",
      "Trainable params: 4782583 (18.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "IMG_FEAT_DIM = C.FEAT_DIM\n",
    "EMB_DIM = C.EMB_DIM\n",
    "LSTM_UNITS = C.LSTM_UNITS\n",
    "\n",
    "#  Image feature input\n",
    "img_input = Input(shape=(IMG_FEAT_DIM,), name=\"image_features\")\n",
    "img_embed = layers.Dense(EMB_DIM, activation='relu')(img_input)  # project to same dim as word embeddings\n",
    "img_embed = layers.RepeatVector(max_len)(img_embed)              # repeat for each time step\n",
    "\n",
    "# Text input\n",
    "txt_input = Input(shape=(max_len,), name=\"text_seq\")\n",
    "txt_embed = layers.Embedding(input_dim=vocab_size, output_dim=EMB_DIM, mask_zero=True)(txt_input)\n",
    "\n",
    "# Combine visual + text embeddings\n",
    "merged = layers.concatenate([img_embed, txt_embed])\n",
    "lstm_out = layers.LSTM(LSTM_UNITS, return_sequences=True)(merged)\n",
    "drop = layers.Dropout(0.4)(lstm_out)\n",
    "outputs = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(drop)\n",
    "\n",
    "# Build the model\n",
    "decoder_model = Model(inputs=[img_input, txt_input], outputs=outputs)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab2d96f3-1daa-485a-9014-6fc109ae75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d6f77bd-65d4-4a13-a0f3-ab2ee5265d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load captions and features\n",
    "with open(C.PROC / \"captions_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "train_data = np.load(C.PROC / \"features_b0_train.npz\")\n",
    "train_features = train_data[\"features\"]\n",
    "train_files = train_data[\"filenames\"]\n",
    "\n",
    "# Map filename → feature vector\n",
    "feat_map = {fn: feat for fn, feat in zip(train_files, train_features)}\n",
    "\n",
    "def caption_to_seq(caption, stoi, max_len):\n",
    "    seq = [stoi.get(w, stoi[\"<unk>\"]) for w in caption.split()]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [0] * (max_len - len(seq))  # pad\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return np.array(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "738309a9-4b87-4f69-8b2f-abc98a70f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def seq2seq_generator(captions, image_ids, feat_map, stoi, max_len, batch_size=64):\n",
    "    \"\"\"\n",
    "    Yields:\n",
    "      inputs: [image_features, input_seq]\n",
    "        - image_features: (batch, IMG_FEAT_DIM)\n",
    "        - input_seq:      (batch, max_len)\n",
    "      targets:\n",
    "        - y:              (batch, max_len)  (sparse integer targets)\n",
    "    \"\"\"\n",
    "    pad_id = 0\n",
    "    unk_id = stoi.get(\"<unk>\", 1)\n",
    "\n",
    "    X1, X2, Y = [], [], []\n",
    "    while True:\n",
    "        for img in image_ids:\n",
    "            if img not in captions:\n",
    "                continue\n",
    "            for cap in captions[img]:\n",
    "                # token ids\n",
    "                seq = [stoi.get(w, unk_id) for w in cap.split()]\n",
    "\n",
    "                # teacher forcing: input is tokens[:-1], target is tokens[1:]\n",
    "                in_seq  = seq[:-1]\n",
    "                out_seq = seq[1:]\n",
    "\n",
    "                in_seq  = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                    [in_seq], maxlen=max_len, padding='post', truncating='post', value=pad_id\n",
    "                )[0]\n",
    "                out_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                    [out_seq], maxlen=max_len, padding='post', truncating='post', value=pad_id\n",
    "                )[0]\n",
    "\n",
    "                X1.append(feat_map[img])\n",
    "                X2.append(in_seq)\n",
    "                Y.append(out_seq)\n",
    "\n",
    "                if len(X1) == batch_size:\n",
    "                    yield ([np.array(X1), np.array(X2)], np.array(Y))\n",
    "                    X1, X2, Y = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ca0e1b5-4ec9-4ebe-ac7f-1b37827f0cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dim from file: 1280\n",
      "Total captions: 32360\n",
      "Steps per epoch: 505\n",
      "X_img: (64, 1280)\n",
      "X_seq: (64, 20)\n",
      "y    : (64, 20)\n",
      "Epoch 1/20\n",
      "505/505 [==============================] - 84s 162ms/step - loss: 4.2788 - accuracy: 0.2635\n",
      "Epoch 2/20\n",
      "505/505 [==============================] - 80s 159ms/step - loss: 3.3553 - accuracy: 0.3442\n",
      "Epoch 3/20\n",
      "505/505 [==============================] - 81s 160ms/step - loss: 3.0549 - accuracy: 0.3702\n",
      "Epoch 4/20\n",
      "505/505 [==============================] - 87s 173ms/step - loss: 2.8547 - accuracy: 0.3892\n",
      "Epoch 5/20\n",
      "505/505 [==============================] - 83s 164ms/step - loss: 2.7002 - accuracy: 0.4064\n",
      "Epoch 6/20\n",
      "505/505 [==============================] - 78s 155ms/step - loss: 2.5750 - accuracy: 0.4210\n",
      "Epoch 7/20\n",
      "505/505 [==============================] - 78s 155ms/step - loss: 2.4621 - accuracy: 0.4356\n",
      "Epoch 8/20\n",
      "505/505 [==============================] - 78s 154ms/step - loss: 2.3661 - accuracy: 0.4479\n",
      "Epoch 9/20\n",
      "505/505 [==============================] - 81s 160ms/step - loss: 2.2793 - accuracy: 0.4601\n",
      "Epoch 10/20\n",
      "505/505 [==============================] - 81s 160ms/step - loss: 2.2019 - accuracy: 0.4705\n",
      "Epoch 11/20\n",
      "505/505 [==============================] - 79s 156ms/step - loss: 2.1309 - accuracy: 0.4808\n",
      "Epoch 12/20\n",
      "505/505 [==============================] - 78s 155ms/step - loss: 2.0684 - accuracy: 0.4901\n",
      "Epoch 13/20\n",
      "505/505 [==============================] - 79s 156ms/step - loss: 2.0087 - accuracy: 0.4993\n",
      "Epoch 14/20\n",
      "505/505 [==============================] - 80s 159ms/step - loss: 1.9576 - accuracy: 0.5076\n",
      "Epoch 15/20\n",
      "505/505 [==============================] - 78s 154ms/step - loss: 1.9035 - accuracy: 0.5154\n",
      "Epoch 16/20\n",
      "505/505 [==============================] - 79s 157ms/step - loss: 1.8543 - accuracy: 0.5241\n",
      "Epoch 17/20\n",
      "505/505 [==============================] - 78s 154ms/step - loss: 1.8061 - accuracy: 0.5315\n",
      "Epoch 18/20\n",
      "505/505 [==============================] - 78s 155ms/step - loss: 1.7590 - accuracy: 0.5391\n",
      "Epoch 19/20\n",
      "505/505 [==============================] - 79s 157ms/step - loss: 1.7177 - accuracy: 0.5467\n",
      "Epoch 20/20\n",
      "505/505 [==============================] - 79s 156ms/step - loss: 1.6792 - accuracy: 0.5536\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "PROC = C.PROC\n",
    "\n",
    "# 1) Load captions and train features (if not already loaded in this session)\n",
    "caps = json.load(open(C.PROC / \"captions_clean.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "train_npz = np.load(C.PROC / \"features_b0_train.npz\")\n",
    "train_features = train_npz[\"features\"]     # shape: (N_train, IMG_FEAT_DIM)\n",
    "train_files = train_npz[\"filenames\"]       # shape: (N_train,)\n",
    "feat_map = {fn: feat for fn, feat in zip(train_files, train_features)}\n",
    "\n",
    "# 2) (Optional sanity) check IMG_FEAT_DIM matches your model\n",
    "print(\"Feature dim from file:\", train_features.shape[1])\n",
    "\n",
    "# 3) Build the generator (make sure you're using *seq2seq_generator*)\n",
    "batch_size=C.BATCH_SIZE\n",
    "train_image_ids = list(feat_map.keys())  # use all train images\n",
    "gen = seq2seq_generator(caps, train_image_ids, feat_map, stoi, max_len, batch_size=batch_size)\n",
    "\n",
    "# 4) Compute steps_per_epoch so one epoch ≈ all captions once\n",
    "total_captions = sum(len(caps.get(img, [])) for img in train_image_ids)\n",
    "steps_per_epoch = max(1, total_captions // batch_size)\n",
    "\n",
    "print(\"Total captions:\", total_captions)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "# 5) (Optional) quick batch shape check before training\n",
    "(X_img, X_seq), y = next(gen)\n",
    "print(\"X_img:\", X_img.shape)  # (batch_size, IMG_FEAT_DIM)\n",
    "print(\"X_seq:\", X_seq.shape)  # (batch_size, max_len)\n",
    "print(\"y    :\", y.shape)      # (batch_size, max_len)\n",
    "\n",
    "# 6) Train\n",
    "history = decoder_model.fit(\n",
    "    seq2seq_generator(caps, train_image_ids, feat_map, stoi, max_len, batch_size=batch_size),\n",
    "    epochs=C.EPOCHS,                 # start small; you can increase later (e.g., 15–20)\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b3a161a-935a-4325-a893-1b8d7fcea28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved successfully at: C:\\Users\\mkalam\\ML_projs\\image_captioning\\models\\caption_decoder.keras\n"
     ]
    }
   ],
   "source": [
    "# === Save trained model ===\n",
    "C.MODELS.mkdir(parents=True, exist_ok=True)  # ensure folder exists\n",
    "model_path = C.MODELS / \"caption_decoder.keras\"\n",
    "decoder_model.save(model_path)\n",
    "\n",
    "print(f\"Model saved successfully at: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c869c31-8dea-4b30-b878-ef6e77d60b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cap_env)",
   "language": "python",
   "name": "cap_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
